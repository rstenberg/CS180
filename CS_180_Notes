Homework 6 * 3 pts%
Weekly Quizzes: 10 * 1 %
Exam 1: 26%
Exam 2: 22%
Exam 3: 24% - March 23rd (Thursday 10th week)
	"Final" is non-cummulative (only last 6 lectures)

Grading: Homework
Goes live to CCLE (Monday 2pm) -> Upload to Gradescope (before next Monday 6:59 pm) -> Homework discussed in section (Friday following the deadline)
Late Submissions = 0 pts
Regrade: Ask on Gradescope within a week
Attemps count: correct: 100%, serious attempt: 75%, reasonable attempt: 50%

Quizzes
Roughly 5 multiple choice questions
Timed quiz
Live on CCLE Wednesday 6:00 pm to Friday 5:59 pm


----------------------- Introduction ------------------------

What is an algorithm?
"An algorithm is a finite, definite, effective procedure, with some input and some output"

Ask two questions for every algorithm:
1) Is the algorithm correct?
2) How efficient is the algorithm?

Must always define efficiency

Want a general theory of efficiency that is:
	1. Simple (mathematically)
	2. Objective (doesn't depend on i5 vs i7 processor)
	3. Captures scalability (input-sizes change)
	4. Predictive of practical performance

How we measure efficiency
Most important resource in computing: TIME
TIME: # of instructions executed in a simple programming language
	Only simple operations (+, *, -, =, if, call)
	Each operation takes one time step
	Each memory access takes one time step
	No fancy stuff built in (add these two matrices, copy this long string, ...)

Things we've dropped from consideration:
	1. Memory hierarchy
		disk, caches, registers have many orders of magnitude differences in access time
	2. Not all instructions take the same time in practice (+, /)
	3. Communication
	4. Different computers have different primitive instructions
However:
	One can usually tune implementations so that the hierarchy, etc., is not a huge factor

Overarching Problem:
	Algorithms can have different running times on different inputs
	Smaller inputs take less time, larger inputs take more time
Solution:
	Measure performance on input size 'n'
	Average-case complexity: Average # steps algorithm takes on inputs of size n
	Worst-case complexity: Max # steps algorithm takes over all inputs of size n

Pros/Cons:
	Average-case:
		Over what probability distribution? (diff people may have different "average" problems)
		Analysis often hard
	Worst-case
		A fast algorithm has a comforting guarantee
		Analysis easier than average-case
		Userful in real-time applications (space shuttle, nuclear reactors)
		May be too pessimistic

Best-case...
	Characterize growth-rate of (worst-case) run time as a function of problem size, up to a constant factor
	Why not try to be more precise?
		Technological variations (computer, compiler, OS, ...) can easily cause 10x difference
		Not necessary to track EXACT run time of algorithm

Complexity:
	Complexity of an algorithm associates a number T(n) with each problem size n
	T(n) = worst-case time taken on problems of size n

Asymptotic Analysis:
	Little o and big O are different
		f(n) = o(g(n)): f(n)/g(n) tends to 0 as n goes to infinity
			ex: f(n) = n;  	g(n) = n^2
		f(n) = omega(g(n)):f(n)/g(n) tends to infinity as n goes to infinity
			ex: f(n) = n^3;	g(n) = n^2.9

	Methodology for comparing run-times
		Given two functions: f,g
		f(n) = O(g(n)): iff there is a constant c>0 such that f(n) is eventually-always at MOST c*g(n)
		f(n) = Omega(f(n))): iff there is a constant c>0 such that f(n) is eventually-always at LEAST c*g(n)
		f(n) = Theta((g(n)): iff both hold, there are constants c1>0, c2>0 such that f(n) is eventually-always at MOST c1*g(n) and at LEAST c2*g(n)

	T(n) vs. O(g(n))
		T(n) 	= exact number of parisions made
		O(g(n)) = upper bound of comparisons made by function g(n)

Polynomial time
	P: running time is O(n^d) for some constant d independent of the input size n
		ex: if constant d = 2, then doubling input size raises run-time by the power of 2

Summary
	Typical goal for algorithm is to find a 
		Reasonably tight (i.e. Theta(n) if possible)
		asymptotic (i.e. O(n) or Theta(n))
		bound <- usually upper bound
		worst case run-time as a function of problem size

Exmaple: Multiplication
	Calculating each row: O(n)
	Adding up each column O(n)
	Total time O(n^2)

----------------------- Divide and Conquer -----------------------

Steps:
	1) Divide Problem into several subproblems
	2) Solve each subproblem recursively
	3) Combine solutions to subproblems into overall solution

Most common usage:
	Divide size n into two subproblems of size n/2 in linear time
	Solve two subproblems recursively
	Combine two solutions into overall solution in linear time
	Run-time: O(n log n)!

Obvious applications:
	Organize an MP3 library
	Display Google PageRank results
	List RSS news items in reverse chronological order

Some problems become easier onces sorted
	Identifying statistical outliers

Non-obvious applications
	Convex hull
	Closest pair of points
	Interval scheduling / interval partitioning
	Minimum spanning trees (Kruskal's algorithm)
	Scheduling

Why mergesort?
	Much faster that "Selection", "insertion", "Bubble"
	Sorting in Perl, Java,, Python, Android:
		- Hybrid of mergesort and others
	Excellent example of divide and conquer principles
	Example:
		1) Input: [ALGORITHMS] 											T(n)
		2) Divide array into two halves [ALGOR] [ITHMS]					O(1)
		3) Recursively sort left half and right half [AGLOR] [HIMST]	2T(n/2)
		4) Merge elements of two halves into one array					O(n)

		Merging:
			Keep track of smallest element in sorted halves
			Insert smallest of two elements into new array
			Repeat until done

	Is Mergesort any good?
		Defn: T(n) = # comparisions made by mergesort in worst-case on array with n elements
		Mergesort recurrence: T(1) = 1
			T(n) 		=		T(n / 2) 	+ 	T(n/2) 			+ 	n
			Total time 	= 	Sort left array + sort right array	+	merging
		Need n comparisons to merge at every level
		2(n/2) = n 		<- merging all (n/2)-element arrays (there are 2)
		4(n/4) = n 		<- merging all (n/4)-element arrays (there are 4)
		8(n/8) = n 		<- merging all (n/8)-element arrays	(there are 8)
		.
		.
		2^k (n/2^k) = n <- merging all (n/2^k)-element arrays (there are 2^k)
		.
		.
		(n/2)2 = n 		<- merging all 2-element arrays (there are (n/2))



// ***************** Master Theorem ****************** //

Master Method:

	Master Theorem: Suppose that T(n) is a function on the nonnegative integers satisfying the recurrence:
		T(n) = aT(n/b) + f(n)
		k = logb(a)

		case 1) f(n) = O(n^(k - e))	e > 0, 	then T(n) = theta(n^k)
			ex: T(n) = 3T(n/2) + n
				a = 3
				b = 2
				f(n) = n, k = log2(3)
				T(n) = theta(n^(log2(3)))
		case 2) f(n) = O(n^k * (log(n))^p T(n) = theta(n^k * (log(n))^p+1
			ex: T(n) = 2T(n/2) + theta(nlogn)
				a = 2
				b = 2
				k = log2(2) = 1
				p = 1
				T(n) = theta(n(log(n)^2)
		case 3) f(n) = O(n^(k + e)) for some constant e>0, 	then T(n) = theta(f(n))
			ex: T(n) = 3T(n/4) + n^5
				a = 3
				b = 4
				k = log4(3)
				T(n) = theta(f(n))

	Solve common divide/conquer recurrences:
		T(n) = aT(n/b) + f(n)
		Terms:
			a>=1 	is the number of subproblems
			b>0 	is the factor by which subproblem size decreases
			f(n) 	is work to divide/merge subproblems
		ex: in mergesort, after first division
			a 		= 2 (arrays)
			b 		= 2 (decreased by factor of b)
			f(n) 	= n (division cost is trivial, merging costs n because every element in each subarray must be itterated through)
	Case 1: Cost of leaves dominates total cost
		ex: T(I) = I.	T(n) = 3T(n/2) + n 		Then, T(n)=theta(n^(log2(3))
			Each level of recursion requires 50% more operations than the last
			Costs per level:
				1) 				3^0(n/2^0)
				2) 				3^1(n/2^1)
				3) 				3^2(n/2^2)
								.......
				i) 				(3^(log2(i)))(n/2^(log2(i))) 
				Final term: 	3^(log2(n)) = n^(log2(3)) 
			Sum of all levels:
				Geometric series
				T(n) =	n * (r^1+log2(n) - 1)/(r - 1)
				multiply by "n" (the merge cost)
	Case 2: Total cost evenly distributed on all levels
		ex: T(I) = I. 	T(n) = 2T(n/2) + n 		Then T(n) = theta(nlogn)
			ex: T(I) = I.	T(n) = 3T(n/2) + n 		Then, T(n)=theta(n^(log2(3))
			Each level of recursion requires the same operations as the last
			Costs per level:
				1) 				2^0(n/2^0)
				2) 				2^1(n/2^1)
				3) 				2^2(n/2^2)
								.......
				i) 				(2^(log2(i)))(n/2^(log2(i))) 
				Final term: 	2^(log2(n)) = n^(log2(2)) 
			Sum of all levels:
				Geometric series
				T(n) = n(log2(n) + 1)
				multiply by "n" (the merge cost)
	Case 3: Total cost dominated by cost at root
		ex: T(I) = I. 	T(n) = 3T(n/4) + n^5 		Then T(n) = theta(n^5)
		Each level of recursion requires much less operations than the last
			Costs per level:
				1) 				3^1 * (n/4^1)^5
				2) 				3^2 * (n/4^2)^5
				3) 				3^3 * (n/4^3)^5
								.......
				i) 				3^i * (n/4^i)^5
				Final term: 	3^(log4(n)) * (n / 4^(log4(n)))^5
			Sum of all levels:
				Geometric series
				T(n) = n(log2(n) + 1)
				n^5 <= T(n) <= (1 + r + r^2 + r3 + ...)n^5 <= (1/ 1-r)n^5




// ***************** Integer Multiplication ****************** //

INPUT: 	Two n-bit numbers a, b in binary
OUTPUT:	(a x b) in binary format

Grade school algorithm O(n^2) operations

Divide and conquer applied to multiplication:
	1) Divide a and b into high and low order bits
		ex: x = [123456]
			x0 = [123]   x1 =[456]
			y = [456789]
			y0 = [456]   y1 =[789]
	2) Multiply four (n/2)-bit integers recursively
		ex: x0 * y0		x0 * y1		x1 * y0		x1 * x1
	3) Shift and add to obtain x * y
		x = 2^(n/2)x1 + x0
		y = 2^(n/2)y1 + y0
		x * y 	= 2^(n/2)x1 + x0 + 2^(n/2)y1 + y0
				= 2^n(x1 * y1) + (2^(n/2) * (x0 * y1) + (x1 * y0))) + (x0 * y0)
	Requires theta(n^2) operations to multiply 2 n-bit integers
	T(n) = 4T(n/2) + O(n)
		- four recurrsive calls 	4T(n/2)
		- three additions			O(n)
	Apply master theorem:
		a = 4
		b = 2
		f(n) = O(n)
		theta(n^2)

	How to cut out one multiplication:
		Must condense:		(x0 * y1) + (x1 * y0)
		Define some terms 	t1 = x1y1	t0 = x0y0	t10 = x1y1 + x0y1 + x1y0 + x0y0
			x0y1 + x1y0 + x1y1 + x0y0 = (x1 + x0)(y1 + y0)   	 <--- must subtract t1 and t0 to get (x0y1 + x1y0)
			T(n) = 2^n(t1) + 2^n(t10 - t1 - t0) + t0
			theta(n^1.585)

	>>>>>>> Karatsuba's algorithm requires O(n^1.585) bit operations to multiply 2 n-bit integers <<<<<<<
	T(n) = 3T(n/2) + O(n)
		- four recursive calls
		- nine adds/subtractions


// ***************** Exponentiation ****************** //

Very useful in number theory, Cryptography

INPUT: Given two numbers a, n
Output: a^n in binary format
	ex: a = 11(3), n = 10
	1110011010101001

Naive-exponentiation(3, n):
	1)	A[0] = 1
	2)	for i = 1: n,
			A[i] = 3 * A[i-1]
		return A
	Runtime O(n^2)
		Each step takes O(i)
		Sum: 	= O(1) + O(1 + 2 + ... + n)
				= O(1) + O(n(n+1)/2)
				= O(n^2)

Fast exponentiation(3, n)
	1) If n = 1, Return 3
	2) Else
		(a) Set A[l] = exponentiate(3, [n/2])
		(b) Set A[r] = exponentiate(3, [n/2])
		(c) return Karatsuba-Multiply(A[l], A[r])
	T(n) = 2T(n/2) + T( n^(log2(3) )

Even faster
	1) If n = 1, Return 3
	2) Else
		(a) Set A[l] = exponentiate(3, [n/2])
		(b) if n == even
			return Karatsuba-Multiply(A[l], A[l])
		(c) if n == odd
			return 3 * Karatsuba-Multiply(A[l], A[l])
	Runtime: O(1) + T(n/2) + O( n^(log2(3) ) + O(n) ----> T(n/2) + O( n^(log2(3)) )
	Proposition: Fast-Exponentiate runs in O( n^1.585 )
		Can prove by applying master theorem




		 
